# Activation functions for Neural Networks

This repository contains the notebook with implentation of most common activation functions. 

## Content
- Binary
- Linear
- Sigmoid
- Tanh
- ReLU
- Leaky ReLU (LReLU)
- Parametric ReLU (PReLU)
- Exponential Linear Unit (eLU)
- ReLU-6
- Softplus
- Softsign
- Softmax
- Gaussian
- Swish

This notebook is linked with the article [What is activation function ?](https://medium.com/@pere.christophe1/what-is-activation-function-1464a629cdca) on [medium.com](https://medium.com/)